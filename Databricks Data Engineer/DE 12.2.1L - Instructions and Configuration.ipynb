{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8d9449d-092d-4e29-ad3d-f1dfd6a2cd4a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"01f3c782-1973-4a69-812a-7f9721099941\"/>\n\n\n\n## End-to-End ETL in the Lakehouse\n\nIn this notebook, you will pull together concepts learned throughout the course to complete an example data pipeline.\n\nThe following is a non-exhaustive list of skills and tasks necessary to successfully complete this exercise:\n* Using Databricks notebooks to write queries in SQL and Python\n* Creating and modifying databases, tables, and views\n* Using Auto Loader and Spark Structured Streaming for incremental data processing in a multi-hop architecture\n* Using Delta Live Table SQL syntax\n* Configuring a Delta Live Table pipeline for continuous processing\n* Using Databricks Jobs to orchestrate tasks from notebooks stored in Repos\n* Setting chronological scheduling for Databricks Jobs\n* Defining queries in Databricks SQL\n* Creating visualizations in Databricks SQL\n* Defining Databricks SQL dashboards to review metrics and results"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80010971-4087-40e5-94be-7d5298237773","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"f9cf3bbc-aa6a-45c2-9d26-a3785e350e1f\"/>\n\n\n## Run Setup\nRun the following cell to reset all the databases and directories associated with this lab."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7465b520-118d-41bd-9788-d752bcf5c0a3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ../../Includes/Classroom-Setup-12.2.1L"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0610c3bd-5e30-416b-8308-cf7363c8d838","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(7 seconds)\n| completed (7 seconds total)\n\nCreating & using the schema \"munirsheikhcloudseekho_0lj9_da_dewd_cap_12\"...(1 seconds)\nPredefined tables in \"munirsheikhcloudseekho_0lj9_da_dewd_cap_12\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12\n| DA.paths.user_db:          dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/database.db\n| DA.paths.datasets:         dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\n| DA.paths.checkpoints:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/_checkpoints\n| DA.paths.stream_path:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/stream\n| DA.paths.storage_location: dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/storage\n\nSetup completed (9 seconds)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(7 seconds)\n| completed (7 seconds total)\n\nCreating & using the schema \"munirsheikhcloudseekho_0lj9_da_dewd_cap_12\"...(1 seconds)\nPredefined tables in \"munirsheikhcloudseekho_0lj9_da_dewd_cap_12\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12\n| DA.paths.user_db:          dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/database.db\n| DA.paths.datasets:         dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\n| DA.paths.checkpoints:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/_checkpoints\n| DA.paths.stream_path:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/stream\n| DA.paths.storage_location: dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/storage\n\nSetup completed (9 seconds)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"3fe92b6e-3e10-4771-8eef-8f4b060dd48f\"/>\n\n\n## Land Initial Data\nSeed the landing zone with some data before proceeding."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68943eff-9239-44e0-a91a-337a042c4382","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.data_factory.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"082a5158-8b99-45da-8b2e-e9259213f6a8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Loading the file 01.json to the dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/stream/01.json\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Loading the file 01.json to the dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/stream/01.json\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"806818f8-e931-45ba-b86f-d65cdf76f215\"/>\n\n\n## Create and Configure a DLT Pipeline\n**NOTE**: The main difference between the instructions here and in previous labs with DLT is that in this instance, we will be setting up our pipeline for **Continuous** execution in **Production** mode."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eaf4dc3b-42c7-486e-a8c9-67e37a8f09b9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.print_pipeline_config()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69c01acf-2e93-4439-81dc-aebc403ccbde","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n        <td><input type=\"text\" value=\"Cap-12-munirsheikhcloudseekho@gmail.com\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n        <td><input type=\"text\" value=\"munirsheikhcloudseekho_0lj9_da_dewd_cap_12\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/storage\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n        <td><input type=\"text\" value=\"/Users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/data-engineering-with-databricks/Solutions/12 - Productionalizing Dashboards and Queries in DBSQL/DE 12.2L - OPTIONAL Capstone/DE 12.2.2L - DLT Task\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Datasets Path:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/stream\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Policy:</td>\n        <td><input type=\"text\" value=\"DBAcademy DLT-Only Policy\" style=\"width:100%\"></td></tr>\n    </table>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n        <td><input type=\"text\" value=\"Cap-12-munirsheikhcloudseekho@gmail.com\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n        <td><input type=\"text\" value=\"munirsheikhcloudseekho_0lj9_da_dewd_cap_12\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/storage\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n        <td><input type=\"text\" value=\"/Users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/data-engineering-with-databricks/Solutions/12 - Productionalizing Dashboards and Queries in DBSQL/DE 12.2L - OPTIONAL Capstone/DE 12.2.2L - DLT Task\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Datasets Path:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/cap_12/stream\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Policy:</td>\n        <td><input type=\"text\" value=\"DBAcademy DLT-Only Policy\" style=\"width:100%\"></td></tr>\n    </table>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"e1663032-caa8-4b99-af1a-3ab27deaf130\"/>\n\n\nSteps:\n1. Click the **Workflows** button on the sidebar.\n1. Select the **Delta Live Tables** tab.\n1. Click **Create Pipeline**.\n1. Leave **Product Edition** as **Advanced**.\n1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **Pipeline Name** provided in the cell above.\n1. For **Notebook Libraries**, use the navigator to locate and select the notebook specified above.\n1. Under **Configuration**, add three configuration parameters:\n   * Click **Add configuration**, set the \"key\" to **spark.master** and the \"value\" to **local[\\*]**.\n   * Click **Add configuration**, set the \"key\" to **datasets_path** and the \"value\" to the value provided in the cell above.\n   * Click **Add configuration**, set the \"key\" to **source** and the \"value\" to the value provided in the cell above.\n1. In the **Target** field, enter the database name provided in the cell above.<br/>\nThis should follow the pattern **`<name>_<hash>_dbacademy_dewd_cap_12`**\n1. In the **Storage location** field, enter the path provided in the cell above.\n1. For **Pipeline Mode**, select **Continuous**\n1. Uncheck the **Enable autoscaling** box.\n1. Set the number of **`workers`** to **`0`** (zero).\n1. Check the **Use Photon Acceleration** box.\n1. For **Channel**, select **Current**\n1. For **Policy**, select the value provided in the cell above.\n1. Click **Create**.\n1. After the UI updates, change from **Development** to **Production** mode\n\nThis should begin the deployment of infrastructure."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7722e50c-fafa-4fac-9141-16e50e2e0f3b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# ANSWER\n \n# This function is provided for students who do not \n# want to work through the exercise of creating the pipeline.\nDA.create_pipeline()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"007e4574-a753-44d1-aaf2-cd396f36778a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4094000743659777>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# This function is provided for students who do not\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# want to work through the exercise of creating the pipeline.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mDA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_pipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m<command-4094000743658799>\u001B[0m in \u001B[0;36mcreate_pipeline\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0;31m# We need to delete the existing pipline so that we can apply updates\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;31m# because some attributes are not mutable after creation.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipelines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     response = self.client.pipelines().create(\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mdelete_by_name\u001B[0;34m(self, pipeline_name)\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdelete_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0;32mimport\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m         \u001B[0mpipeline\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete_by_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pipeline_id\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mget_by_name\u001B[0;34m(self, pipeline_name)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m                 \u001B[0mpipeline_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pipeline_id\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mlist\u001B[0;34m(self, max_results)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_results\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mpipelines\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"GET\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"{self.base_uri}?max_results={max_results}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/rest/common.py\u001B[0m in \u001B[0;36mapi\u001B[0;34m(self, _http_method, _endpoint_path, _data, _expected, _result_type, _base_url, **data)\u001B[0m\n\u001B[1;32m    189\u001B[0m             \u001B[0;31m# if self.verbose: print(json.dumps(data, indent=4))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    190\u001B[0m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_http_method\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 191\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_raise_for_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_expected\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    192\u001B[0m         \u001B[0;31m# TODO: Should we really return None on errors?  Kept for now for backwards compatibility.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    193\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m200\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/rest/common.py\u001B[0m in \u001B[0;36m_raise_for_status\u001B[0;34m(response, expected)\u001B[0m\n\u001B[1;32m    282\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;36m400\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m500\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0me\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDatabricksApiException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhttp_exception\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mdeprecated\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreason\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Use ApiClient.api\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"error\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mHTTPError\u001B[0m: 503 Server Error: Service Unavailable for url: https://community.cloud.databricks.com/api/2.0/pipelines?max_results=100\n Response from server: \n { 'error_code': 'TEMPORARILY_UNAVAILABLE',\n  'message': 'The service at /api/2.0/pipelines is temporarily unavailable. '\n             'Please try again later.'}","errorSummary":"<span class='ansi-red-fg'>HTTPError</span>: 503 Server Error: Service Unavailable for url: https://community.cloud.databricks.com/api/2.0/pipelines?max_results=100\n Response from server: \n { 'error_code': 'TEMPORARILY_UNAVAILABLE',\n  'message': 'The service at /api/2.0/pipelines is temporarily unavailable. '\n             'Please try again later.'}","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4094000743659777>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# This function is provided for students who do not\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# want to work through the exercise of creating the pipeline.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mDA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_pipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m<command-4094000743658799>\u001B[0m in \u001B[0;36mcreate_pipeline\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0;31m# We need to delete the existing pipline so that we can apply updates\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;31m# because some attributes are not mutable after creation.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipelines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     response = self.client.pipelines().create(\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mdelete_by_name\u001B[0;34m(self, pipeline_name)\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdelete_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0;32mimport\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m         \u001B[0mpipeline\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete_by_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pipeline_id\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mget_by_name\u001B[0;34m(self, pipeline_name)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m                 \u001B[0mpipeline_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pipeline_id\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mlist\u001B[0;34m(self, max_results)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_results\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mpipelines\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"GET\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"{self.base_uri}?max_results={max_results}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/rest/common.py\u001B[0m in \u001B[0;36mapi\u001B[0;34m(self, _http_method, _endpoint_path, _data, _expected, _result_type, _base_url, **data)\u001B[0m\n\u001B[1;32m    189\u001B[0m             \u001B[0;31m# if self.verbose: print(json.dumps(data, indent=4))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    190\u001B[0m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_http_method\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 191\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_raise_for_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_expected\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    192\u001B[0m         \u001B[0;31m# TODO: Should we really return None on errors?  Kept for now for backwards compatibility.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    193\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m200\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-72d7d446-c770-4619-97e0-133dc74ecaeb/lib/python3.9/site-packages/dbacademy/rest/common.py\u001B[0m in \u001B[0;36m_raise_for_status\u001B[0;34m(response, expected)\u001B[0m\n\u001B[1;32m    282\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;36m400\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m500\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0me\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDatabricksApiException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhttp_exception\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mdeprecated\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreason\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Use ApiClient.api\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"error\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mHTTPError\u001B[0m: 503 Server Error: Service Unavailable for url: https://community.cloud.databricks.com/api/2.0/pipelines?max_results=100\n Response from server: \n { 'error_code': 'TEMPORARILY_UNAVAILABLE',\n  'message': 'The service at /api/2.0/pipelines is temporarily unavailable. '\n             'Please try again later.'}"]}}],"execution_count":0},{"cell_type":"code","source":["DA.validate_pipeline_config()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f06c9d67-97e7-42c4-967a-41a76c6b71d3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"6c8bd13c-938a-4283-b15a-bc1a598fb070\"/>\n\n\n## Schedule a Notebook Job\n\nOur DLT pipeline is setup to process data as soon as it arrives. \n\nWe'll schedule a notebook to land a new batch of data each minute so we can see this functionality in action.\n\nBefore we start run the following cell to get the values used in this step."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"acc0d5c0-d1a7-4f7d-a949-33b9aaa1b246","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.print_job_config()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"118b9f2e-43ee-4ef9-8641-b753431addc2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"df989e07-97d4-4a34-9729-fad02399a908\"/>\n\n\nSteps:\n1. Click the **Workflows** button on the sidebar\n1. Select the **Jobs** tab.\n1. Click the blue **Create Job** button\n1. Configure the task:\n    1. Enter **Land-Data** for the task name\n    1. For **Type**, select **Notebook**\n    1. For **Path**, select the **Notebook Path** value provided in the cell above\n    1. From the **Cluster** dropdown, under **Existing All Purpose Clusters**, select your cluster\n    1. Click **Create**\n1. In the top-left of the screen rename the job (not the task) from **`Land-Data`** (the defaulted value) to the **Job Name** provided for you in the previous cell.    \n\n<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: When selecting your all purpose cluster, you will get a warning about how this will be billed as all purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce050e02-534a-4320-96e5-4f0c0370bc81","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"3994f3ee-e335-48c7-8770-64e1ef0dfab7\"/>\n\n\n## Set a Chronological Schedule for your Job\nSteps:\n1. Locate the **Schedule** section in the side panel on the right.\n1. Click on the **Edit schedule** button to explore scheduling options.\n1. Change the **Schedule type** field from **Manual (Paused)** to **Scheduled**, which will bring up a chron scheduling UI.\n1. Set the schedule to update **Every 2**, **Minutes** from **0 minutes past the hour** \n1. Click **Save**\n\n**NOTE**: If you wish, you can click **Run now** to trigger the first run, or wait until the top of the next minute to make sure your scheduling has worked successfully."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15626794-5d99-4879-8638-1ea507ecb127","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# ANSWER\n\n# This function is provided for students who do not \n# want to work through the exercise of creating the job.\nDA.create_job()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"93c7ee37-3ef5-4b5c-879f-def432fcaf25","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["DA.validate_job_config()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c6a3880-f601-4ed5-90b2-0ee604e781ed","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# ANSWER\n\n# This function is provided to start the job and  \n# block until it has completed, canceled or failed\nDA.start_job()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7aaeaf45-d2d6-4fc9-9e0e-b360c452db46","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"30df4ffa-22b9-4e2c-b8d8-54aa09a8d4ed\"/>\n\n\n## Register DLT Event Metrics for Querying with DBSQL\n\nThe following cell prints out SQL statements to register the DLT event logs to your target database for querying in DBSQL.\n\nExecute the output code with the DBSQL Query Editor to register these tables and views. \n\nExplore each and make note of the logged event metrics."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0d7a44c-72f2-4f6a-afb2-e93f2df36e32","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.generate_register_dlt_event_metrics_sql()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"17168080-c599-46cd-891c-07ae4cc3cc94","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"e035ddc7-4af9-4e9c-81f8-530e8db7c504\"/>\n\n\n## Define a Query on the Gold Table\n\nThe **daily_patient_avg** table is automatically updated each time a new batch of data is processed through the DLT pipeline. Each time a query is executed against this table, DBSQL will confirm if there is a newer version and then materialize results from the newest available version.\n\nRun the following cell to print out a query with your database name. Save this as a DBSQL query."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40900e65-a50b-4367-9a15-eac8b5e157bf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.generate_daily_patient_avg()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d5ee4122-3320-4e5d-a259-22179d24bea7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"679db36c-b257-4248-b2fe-56b85099d0b9\"/>\n\n\n## Add a Line Plot Visualization\n\nTo track trends in patient averages over time, create a line plot and add it to a new dashboard.\n\nCreate a line plot with the following settings:\n* **X Column**: **`date`**\n* **Y Column**: **`avg_heartrate`**\n* **Group By**: **`name`**\n\nAdd this visualization to a dashboard."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c12d90d9-7654-4836-a06b-21a6452bef73","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"7351e179-68f8-4091-a6ee-647974f010ce\"/>\n\n\n## Track Data Processing Progress\n\nThe code below extracts the **`flow_name`**, **`timestamp`**, and **`num_output_rows`** from the DLT event logs.\n\nSave this query in DBSQL, then define a bar plot visualization that shows:\n* **X Column**: **`timestamp`**\n* **Y Column**: **`num_output_rows`**\n* **Group By**: **`flow_name`**\n\nAdd your visualization to your dashboard."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e834a288-fbde-4902-bdd6-c394da9c6edd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.generate_visualization_query()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d958df8b-984c-449e-9df3-d74c24ca4398","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"5f94b102-d42e-40f1-8253-c14cbf86d717\"/>\n\n\n## Refresh your Dashboard and Track Results\n\nThe **Land-Data** notebook scheduled with Jobs above has 12 batches of data, each representing a month of recordings for our small sampling of patients. As configured per our instructions, it should take just over 20 minutes for all of these batches of data to be triggered and processed (we scheduled the Databricks Job to run every 2 minutes, and batches of data will process through our pipeline very quickly after initial ingestion).\n\nRefresh your dashboard and review your visualizations to see how many batches of data have been processed. (If you followed the instructions as outlined here, there should be 12 distinct flow updates tracked by your DLT metrics.) If all source data has not yet been processed, you can go back to the Databricks Jobs UI and manually trigger additional batches."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"84bdaf68-22b9-41f9-8eda-251fe395ce3a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"b61bf387-2c1b-4ae6-8968-c4189beb477f\"/>\n\n\n\nWith everything configured, you can now continue to the final part of your lab in the notebook [DE 12.2.4L - Final Steps]($./DE 12.2.4L - Final Steps)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a74b3288-0bf3-42c8-afa3-1f459a79c320","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfd0141b-55a3-4569-b291-0b790b6423ce","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 12.2.1L - Instructions and Configuration","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4094000743659767}},"nbformat":4,"nbformat_minor":0}
