{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e13eb1ed-e157-4460-866c-a6c054119c28","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"2eb97b71-b2ab-4b68-afdc-1663ec49e9d4\"/>\n\n\n# Lab: Migrating SQL Notebooks to Delta Live Tables\n\nThis notebook describes the overall structure for the lab exercise, configures the environment for the lab, provides simulated data streaming, and performs cleanup once you are done. A notebook like this is not typically needed in a production pipeline scenario.\n\n## Learning Objectives\nBy the end of this lab, you should be able to:\n* Convert existing data pipelines to Delta Live Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0687fa7-6224-4e00-be9a-84a499fb8954","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"782da0e9-5fc2-4deb-b7a4-939af49e38ed\"/>\n\n\n## Datasets Used\n\nThis demo uses simplified artificially generated medical data. The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n\n#### Recordings\nThe main dataset uses heart rate recordings from medical devices delivered in the JSON format. \n\n| Field | Type |\n| --- | --- |\n| device_id | int |\n| mrn | long |\n| time | double |\n| heartrate | double |\n\n#### PII\nThese data will later be joined with a static table of patient information stored in an external system to identify patients by name.\n\n| Field | Type |\n| --- | --- |\n| mrn | long |\n| name | string |"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cee7cfcc-4338-4674-ba2a-643500cc0114","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"b691e21b-24a5-46bc-97d8-a43e9ae6e268\"/>\n\n\n## Getting Started\n\nBegin by running the following cell to configure the lab environment."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"29c64a30-b714-4670-a400-3b07b91c943b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ../../Includes/Classroom-Setup-08.2.1L"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c797d2cd-f87a-4a38-84c9-2142fb23bfaa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(7 seconds)\n| completed (7 seconds total)\n\nCreating & using the schema \"munirsheikhcloudseekho_0lj9_da_dewd_dlt_lab_82\"...(0 seconds)\nLoading the file 01.json to the dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/stream/01.json\nPredefined tables in \"munirsheikhcloudseekho_0lj9_da_dewd_dlt_lab_82\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82\n| DA.paths.user_db:          dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/database.db\n| DA.paths.datasets:         dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\n| DA.paths.checkpoints:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/_checkpoints\n| DA.paths.stream_path:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/stream\n| DA.paths.storage_location: dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/storage\n\nSetup completed (10 seconds)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(7 seconds)\n| completed (7 seconds total)\n\nCreating & using the schema \"munirsheikhcloudseekho_0lj9_da_dewd_dlt_lab_82\"...(0 seconds)\nLoading the file 01.json to the dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/stream/01.json\nPredefined tables in \"munirsheikhcloudseekho_0lj9_da_dewd_dlt_lab_82\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82\n| DA.paths.user_db:          dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/database.db\n| DA.paths.datasets:         dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\n| DA.paths.checkpoints:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/_checkpoints\n| DA.paths.stream_path:      dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/stream\n| DA.paths.storage_location: dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/storage\n\nSetup completed (10 seconds)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"c68290ac-56ad-4d6e-afec-b0a61c35386f\"/>\n\n\n## Land Initial Data\nSeed the landing zone with more data before proceeding.\n\nYou will re-run this command to land additional data later."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59794945-a42d-41c0-920a-5a56c61af6c6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.data_factory.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9edce61-4c4b-4c2d-9272-6bbad8cbbf67","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Loading the file 02.json to the dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/stream/02.json\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Loading the file 02.json to the dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/stream/02.json\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"7cb98302-06c2-4384-bdf7-2260cbf2662d\"/>\n\n\nExecute the following cell to print out values that will be used during the following configuration steps."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"97462f3c-2a50-4815-9060-894835fd50f4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.print_pipeline_config()    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5878ccd4-a060-4a1e-841d-cad51f6422de","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n        <td><input type=\"text\" value=\"DLT-Lab-82L-munirsheikhcloudseekho@gmail.com\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n        <td><input type=\"text\" value=\"munirsheikhcloudseekho_0lj9_da_dewd_dlt_lab_82\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/storage\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n        <td><input type=\"text\" value=\"/Users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/data-engineering-with-databricks/Solutions/08 - Delta Live Tables/DE 8.2 - DLT Lab/DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/stream\" style=\"width:100%\"></td>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Datasets Path:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Policy:</td>\n        <td><input type=\"text\" value=\"DBAcademy DLT-Only Policy\" style=\"width:100%\"></td></tr>\n    </table>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n        <td><input type=\"text\" value=\"DLT-Lab-82L-munirsheikhcloudseekho@gmail.com\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Target:</td>\n        <td><input type=\"text\" value=\"munirsheikhcloudseekho_0lj9_da_dewd_dlt_lab_82\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Storage Location:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/storage\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Notebook Path:</td>\n        <td><input type=\"text\" value=\"/Users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/data-engineering-with-databricks/Solutions/08 - Delta Live Tables/DE 8.2 - DLT Lab/DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Source:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/dlt_lab_82/stream\" style=\"width:100%\"></td>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Datasets Path:</td>\n        <td><input type=\"text\" value=\"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Policy:</td>\n        <td><input type=\"text\" value=\"DBAcademy DLT-Only Policy\" style=\"width:100%\"></td></tr>\n    </table>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"784d3bc4-5c4e-4ef8-ab56-3ebaa92238b0\"/>\n\n\n## Create and Configure a Pipeline\n\n1. Click the **Workflows** button on the sidebar.\n1. Select the **Delta Live Tables** tab.\n1. Click **Create Pipeline**.\n1. Leave **Product Edition** as **Advanced**.\n1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **Pipeline Name** provided in the cell above.\n1. For **Notebook Libraries**, use the navigator to locate and select the notebook specified above.\n1. Under **Configuration**, add three configuration parameters:\n   * Click **Add configuration**, set the \"key\" to **spark.master** and the \"value\" to **local[\\*]**.\n   * Click **Add configuration**, set the \"key\" to **datasets_path** and the \"value\" to the value provided in the cell above.\n   * Click **Add configuration**, set the \"key\" to **source** and the \"value\" to the value provided in the cell above.\n1. In the **Target** field, enter the database name provided in the cell above.<br/>\nThis should follow the pattern **`<name>_<hash>_dbacademy_dewd_dlt_lab_82`**\n1. In the **Storage location** field, enter the path provided in the cell above.\n1. For **Pipeline Mode**, select **Triggered**.\n1. Uncheck the **Enable autoscaling** box.\n1. Set the number of **`workers`** to **`0`** (zero).\n1. Check the **Use Photon Acceleration** box.\n1. For **Channel**, select **Current**\n1. For **Policy**, select the value provided in the cell above.\n\nFinally, click **Create**."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"204180d7-3b96-43d5-bb76-afe2c94dd294","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# ANSWER\n\n# This function is provided for those students that do not \n# want to work through the exercise of creating the pipeline.\nDA.create_pipeline()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a479e49-a195-4293-b242-d997c22170b9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4094000743659699>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# This function is provided for those students that do not\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# want to work through the exercise of creating the pipeline.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mDA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_pipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m<command-4094000743658906>\u001B[0m in \u001B[0;36mcreate_pipeline\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0;31m# We need to delete the existing pipline so that we can apply updates\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;31m# because some attributes are not mutable after creation.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipelines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     response = self.client.pipelines().create(\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mdelete_by_name\u001B[0;34m(self, pipeline_name)\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdelete_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0;32mimport\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m         \u001B[0mpipeline\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete_by_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pipeline_id\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mget_by_name\u001B[0;34m(self, pipeline_name)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m                 \u001B[0mpipeline_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pipeline_id\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mlist\u001B[0;34m(self, max_results)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_results\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mpipelines\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"GET\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"{self.base_uri}?max_results={max_results}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/rest/common.py\u001B[0m in \u001B[0;36mapi\u001B[0;34m(self, _http_method, _endpoint_path, _data, _expected, _result_type, _base_url, **data)\u001B[0m\n\u001B[1;32m    189\u001B[0m             \u001B[0;31m# if self.verbose: print(json.dumps(data, indent=4))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    190\u001B[0m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_http_method\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 191\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_raise_for_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_expected\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    192\u001B[0m         \u001B[0;31m# TODO: Should we really return None on errors?  Kept for now for backwards compatibility.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    193\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m200\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/rest/common.py\u001B[0m in \u001B[0;36m_raise_for_status\u001B[0;34m(response, expected)\u001B[0m\n\u001B[1;32m    282\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;36m400\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m500\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0me\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDatabricksApiException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhttp_exception\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mdeprecated\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreason\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Use ApiClient.api\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"error\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mHTTPError\u001B[0m: 503 Server Error: Service Unavailable for url: https://community.cloud.databricks.com/api/2.0/pipelines?max_results=100\n Response from server: \n { 'error_code': 'TEMPORARILY_UNAVAILABLE',\n  'message': 'The service at /api/2.0/pipelines is temporarily unavailable. '\n             'Please try again later.'}","errorSummary":"<span class='ansi-red-fg'>HTTPError</span>: 503 Server Error: Service Unavailable for url: https://community.cloud.databricks.com/api/2.0/pipelines?max_results=100\n Response from server: \n { 'error_code': 'TEMPORARILY_UNAVAILABLE',\n  'message': 'The service at /api/2.0/pipelines is temporarily unavailable. '\n             'Please try again later.'}","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4094000743659699>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# This function is provided for those students that do not\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# want to work through the exercise of creating the pipeline.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mDA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_pipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m<command-4094000743658906>\u001B[0m in \u001B[0;36mcreate_pipeline\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0;31m# We need to delete the existing pipline so that we can apply updates\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;31m# because some attributes are not mutable after creation.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipelines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     response = self.client.pipelines().create(\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mdelete_by_name\u001B[0;34m(self, pipeline_name)\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdelete_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0;32mimport\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m         \u001B[0mpipeline\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete_by_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pipeline_id\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mget_by_name\u001B[0;34m(self, pipeline_name)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_by_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mpipeline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mpipeline_name\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m                 \u001B[0mpipeline_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pipeline_id\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/dbrest/pipelines.py\u001B[0m in \u001B[0;36mlist\u001B[0;34m(self, max_results)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_results\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mpipelines\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclient\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"GET\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"{self.base_uri}?max_results={max_results}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/rest/common.py\u001B[0m in \u001B[0;36mapi\u001B[0;34m(self, _http_method, _endpoint_path, _data, _expected, _result_type, _base_url, **data)\u001B[0m\n\u001B[1;32m    189\u001B[0m             \u001B[0;31m# if self.verbose: print(json.dumps(data, indent=4))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    190\u001B[0m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_http_method\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdumps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 191\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_raise_for_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_expected\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    192\u001B[0m         \u001B[0;31m# TODO: Should we really return None on errors?  Kept for now for backwards compatibility.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    193\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m200\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-04178219-f2b7-4f4c-b837-996296a5a787/lib/python3.9/site-packages/dbacademy/rest/common.py\u001B[0m in \u001B[0;36m_raise_for_status\u001B[0;34m(response, expected)\u001B[0m\n\u001B[1;32m    282\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;36m400\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m500\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0me\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDatabricksApiException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhttp_exception\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mdeprecated\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreason\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"Use ApiClient.api\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"error\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mHTTPError\u001B[0m: 503 Server Error: Service Unavailable for url: https://community.cloud.databricks.com/api/2.0/pipelines?max_results=100\n Response from server: \n { 'error_code': 'TEMPORARILY_UNAVAILABLE',\n  'message': 'The service at /api/2.0/pipelines is temporarily unavailable. '\n             'Please try again later.'}"]}}],"execution_count":0},{"cell_type":"code","source":["DA.validate_pipeline_config()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cbb31052-87ec-4772-a7e6-cf1e00e13b99","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"3340e93d-1fad-4549-bf79-ec239b1d59d4\"/>\n\n\n## Open and Complete DLT Pipeline Notebook\n\nYou will perform your work in the companion notebook [DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab]($./DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab),<br/>\nwhich you will ultimately deploy as a pipeline.\n\nOpen the Notebook and, following the guidelines provided therein, fill in the cells where prompted to<br/>\nimplement a multi-hop architecture similar to the one we worked with in the previous section."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"65464add-64b1-472e-9b95-8ca4416de1ef","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"90a66079-16f8-4503-ab48-840cbdd07914\"/>\n\n\n## Run your Pipeline\n\nSelect **Development** mode, which accelerates the development lifecycle by reusing the same cluster across runs.<br/>\nIt will also turn off automatic retries when jobs fail.\n\nClick **Start** to begin the first update to your table.\n\nDelta Live Tables will automatically deploy all the necessary infrastructure and resolve the dependencies between all datasets.\n\n**NOTE**: The first table update may take several minutes as relationships are resolved and infrastructure deploys."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"57aad221-b35b-437e-b69d-ebe9e466cae2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# ANSWER\n\n# This function is provided to start the pipeline and  \n# block until it has completed, is cancelled or failed\nDA.start_pipeline()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c2989e4-8d2d-45a1-8ff3-f994a34a53d6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"d1797d22-692c-43ce-b146-1e0248e65da3\"/>\n\n\n## Troubleshooting Code in Development Mode\n\nDon't despair if your pipeline fails the first time. Delta Live Tables is in active development, and error messages are improving all the time.\n\nBecause relationships between tables are mapped as a DAG, error messages will often indicate that a dataset isn't found.\n\nLet's consider our DAG below:\n\n<img src=\"https://files.training.databricks.com/images/dlt-dag.png\">\n\nIf the error message **`Dataset not found: 'recordings_parsed'`** is raised, there may be several culprits:\n1. The logic defining **`recordings_parsed`** is invalid\n1. There is an error reading from **`recordings_bronze`**\n1. A typo exists in either **`recordings_parsed`** or **`recordings_bronze`**\n\nThe safest way to identify the culprit is to iteratively add table/view definitions back into your DAG starting from your initial ingestion tables. You can simply comment out later table/view definitions and uncomment these between runs."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c70b9dee-348d-4eec-b36d-2beaed631af7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"676f75e8-9406-4177-830d-1c3fceae4e04","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 8.2.1L - Lab Instructions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4094000743659688}},"nbformat":4,"nbformat_minor":0}
