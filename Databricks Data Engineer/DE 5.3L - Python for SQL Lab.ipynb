{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80e7691b-00c1-4138-a204-69ddf047a5f4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"358d2c22-9d78-4888-a7ec-54b7d5f3db64\"/>\n\n\n# Just Enough Python for Databricks SQL Lab\n\n## Learning Objectives\nBy the end of this lab, you should be able to:\n* Review basic Python code and describe expected outcomes of code execution\n* Reason through control flow statements in Python functions\n* Add parameters to a SQL query by wrapping it in a Python function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e349e048-6afe-454e-bfe2-57e9f32236d1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-05.3L"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7015423e-9c85-4e25-bbd0-116aeedbe89f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Resetting the learning environment:\n| removing the working directory \"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks\"...(1 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(7 seconds)\n| completed (7 seconds total)\n\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\n| DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/_checkpoints\n\nSetup completed (9 seconds)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Resetting the learning environment:\n| removing the working directory \"dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks\"...(1 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(7 seconds)\n| completed (7 seconds total)\n\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\n| DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/munirsheikhcloudseekho@gmail.com/data-engineering-with-databricks/_checkpoints\n\nSetup completed (9 seconds)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"97cba873-1459-478f-831b-b52fc54265eb\"/>\n\n\n# Reviewing Python Basics\n\nIn the previous notebook, we briefly explored using **`spark.sql()`** to execute arbitrary SQL commands from Python.\n\nLook at the following 3 cells. Before executing each cell, identify:\n1. The expected output of cell execution\n1. What logic is being executed\n1. Changes to the resultant state of the environment\n\nThen execute the cells, compare the results to your expectations, and see the explanations below."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e83912c0-ca0d-4cf0-a2bc-63519ac9fa02","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["course = \"dewd\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ebad5562-bb93-4450-ad3f-cd2cfca2618e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(f\"SELECT '{course}' AS course_name\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"348d208c-d098-419f-8b90-a78651713f5e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: DataFrame[course_name: string]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: DataFrame[course_name: string]"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.sql(f\"SELECT '{course}' AS course_name\")\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f334921-ae36-43b2-b6b3-27fad7bf4bad","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dewd"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"course_name","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>course_name</th></tr></thead><tbody><tr><td>dewd</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"bc8fda28-92ad-4cd5-aa24-34022269698a\"/>\n\n\n1. **Cmd 5** assigns a string to a variable. When a variable assignment is successful, no output is displayed to the notebook. A new variable is added to the current execution environment.\n1. **Cmd 6** executes a SQL query and displays the schema for the DataFrame alongside the word **`DataFrame`**. In this case, the SQL query is just to select a string, so no changes to our environment occur. \n1. **Cmd 7** executes the same SQL query and displays the output of the DataFrame. This combination of **`display()`** and **`spark.sql()`** most closely mirrors executing logic in a **`%sql`** cell; the results will always be printed in a formatted table, assuming results are returned by the query; some queries will instead manipulate tables or databases, in which case the word **`OK`** will print to show successful execution. In this case, no changes to our environment occur from running this code."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35f4a45e-ef08-4ed7-9105-941f0b264cc3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"ef0b350e-c470-4e89-9617-948e49dd1710\"/>\n\n\n## Setting Up a Development Environment\n\nThroughout this course, we use logic similar to the following cell to capture information about the user currently executing the notebook and create an isolated development database.\n\nThe **`re`** library is the <a href=\"https://docs.python.org/3/library/re.html\" target=\"_blank\">standard Python library for regex</a>.\n\nDatabricks SQL has a special method to capture the username of the **`current_user()`**; and the **`.first()[0]`** code is a quick hack to capture the first row of the first column of a query executed with **`spark.sql()`** (in this case, we do this safely knowing that there will only be 1 row and 1 column).\n\nAll other logic below is just string formatting."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"84da5298-db97-41ee-b206-113a443fdce1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import re\n\nusername = spark.sql(\"SELECT current_user()\").first()[0]\nclean_username = re.sub(\"[^a-zA-Z0-9]\", \"_\", username)\nschema_name = f\"dbacademy_{clean_username}_{course}_5_3l\"\nworking_dir = f\"dbfs:/user/{username}/dbacademy/{course}/5.3l\"\n\nprint(f\"username:    {username}\")\nprint(f\"schema_name:     {schema_name}\")\nprint(f\"working_dir: {working_dir}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19909dac-8e03-4638-b326-37b7c5a77480","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"username:    munirsheikhcloudseekho@gmail.com\nschema_name:     dbacademy_munirsheikhcloudseekho_gmail_com_dewd_5_3l\nworking_dir: dbfs:/user/munirsheikhcloudseekho@gmail.com/dbacademy/dewd/5.3l\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["username:    munirsheikhcloudseekho@gmail.com\nschema_name:     dbacademy_munirsheikhcloudseekho_gmail_com_dewd_5_3l\nworking_dir: dbfs:/user/munirsheikhcloudseekho@gmail.com/dbacademy/dewd/5.3l\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"1273f7a3-823a-4b1f-914a-ce6eaaa867b3\"/>\n\n\nBelow, we add a simple control flow statement to this logic to create and use this user-specific database. \n\nOptionally, we will reset this database and drop all of the contents on repeat execution. (Note the the default value for the parameter **`reset`** is **`True`**)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b9eab17-cf5b-4a69-9f9f-a944c977c566","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def create_database(course, reset=True):\n    import re\n\n    username = spark.sql(\"SELECT current_user()\").first()[0]\n    clean_username = re.sub(\"[^a-zA-Z0-9]\", \"_\", username)\n    schema_name = f\"dbacademy_{clean_username}_{course}_5_3l\"\n    working_dir = f\"dbfs:/user/{username}/dbacademy/{course}/5.3l\"\n\n    print(f\"username:    {username}\")\n    print(f\"schema_name: {schema_name}\")\n    print(f\"working_dir: {working_dir}\")\n\n    if reset:\n        spark.sql(f\"DROP DATABASE IF EXISTS {schema_name} CASCADE\")\n        dbutils.fs.rm(working_dir, True)\n        \n    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name} LOCATION '{working_dir}/{schema_name}.db'\")\n    spark.sql(f\"USE {schema_name}\")\n    \ncreate_database(course)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a4c9227-edd2-45ee-bbc8-0b4eeb91b04e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"username:    munirsheikhcloudseekho@gmail.com\nschema_name: dbacademy_munirsheikhcloudseekho_gmail_com_dewd_5_3l\nworking_dir: dbfs:/user/munirsheikhcloudseekho@gmail.com/dbacademy/dewd/5.3l\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["username:    munirsheikhcloudseekho@gmail.com\nschema_name: dbacademy_munirsheikhcloudseekho_gmail_com_dewd_5_3l\nworking_dir: dbfs:/user/munirsheikhcloudseekho@gmail.com/dbacademy/dewd/5.3l\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"cfa0adf3-cc23-4ba1-8daf-2c70af7fa079\"/>\n\n\nWhile this logic as defined is geared toward isolating students in shared workspaces for instructional purposes, the same basic design could be leveraged for testing new logic in an isolated environment before pushing to production."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8cf73d37-a01a-407c-ae66-4504ee773577","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"1c994e19-2b72-45c3-a174-8a7e21701688\"/>\n\n\n## Handling Errors Gracefully\n\nReview the logic in the function below.\n\nNote that we've just declared a new database that currently contains no tables."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"26607240-6973-4a71-8d02-a6903a19cb2c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def query_or_make_demo_table(table_name):\n    try:\n        display(spark.sql(f\"SELECT * FROM {table_name}\"))\n        print(f\"Displayed results for the table {table_name}\")\n        \n    except:\n        spark.sql(f\"CREATE TABLE {table_name} (id INT, name STRING, value DOUBLE, state STRING)\")\n        spark.sql(f\"\"\"INSERT INTO {table_name}\n                      VALUES (1, \"Yve\", 1.0, \"CA\"),\n                             (2, \"Omar\", 2.5, \"NY\"),\n                             (3, \"Elia\", 3.3, \"OH\"),\n                             (4, \"Rebecca\", 4.7, \"TX\"),\n                             (5, \"Ameena\", 5.3, \"CA\"),\n                             (6, \"Ling\", 6.6, \"NY\"),\n                             (7, \"Pedro\", 7.1, \"KY\")\"\"\")\n        \n        display(spark.sql(f\"SELECT * FROM {table_name}\"))\n        print(f\"Created the table {table_name}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69135596-f938-4289-a1c8-eaa111ca1c7a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"5a449d08-9811-4b0d-9004-74b8bb04eef5\"/>\n\n\nTry to identify the following before executing the next cell:\n1. The expected output of cell execution\n1. What logic is being executed\n1. Changes to the resultant state of the environment"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"750400dc-ba6e-4234-8701-7f3afe1641b3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["query_or_make_demo_table(\"demo_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f4ac03dd-525b-4c5e-baa4-f7a817faf98b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"Yve",1.0,"CA"],[2,"Omar",2.5,"NY"],[3,"Elia",3.3,"OH"],[4,"Rebecca",4.7,"TX"],[5,"Ameena",5.3,"CA"],[6,"Ling",6.6,"NY"],[7,"Pedro",7.1,"KY"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"value","type":"\"double\"","metadata":"{}"},{"name":"state","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>value</th><th>state</th></tr></thead><tbody><tr><td>1</td><td>Yve</td><td>1.0</td><td>CA</td></tr><tr><td>2</td><td>Omar</td><td>2.5</td><td>NY</td></tr><tr><td>3</td><td>Elia</td><td>3.3</td><td>OH</td></tr><tr><td>4</td><td>Rebecca</td><td>4.7</td><td>TX</td></tr><tr><td>5</td><td>Ameena</td><td>5.3</td><td>CA</td></tr><tr><td>6</td><td>Ling</td><td>6.6</td><td>NY</td></tr><tr><td>7</td><td>Pedro</td><td>7.1</td><td>KY</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Created the table demo_table\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Created the table demo_table\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"8ddb2ea1-9e4e-4ac7-a369-ff984114653f\"/>\n\n\nNow answer the same three questions before running the same query below."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"783cc61b-0376-479c-a2f1-487ea83e698f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["query_or_make_demo_table(\"demo_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4302817-3611-4514-b87f-dd1bd436c745","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"Yve",1.0,"CA"],[2,"Omar",2.5,"NY"],[3,"Elia",3.3,"OH"],[4,"Rebecca",4.7,"TX"],[5,"Ameena",5.3,"CA"],[6,"Ling",6.6,"NY"],[7,"Pedro",7.1,"KY"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"value","type":"\"double\"","metadata":"{}"},{"name":"state","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>value</th><th>state</th></tr></thead><tbody><tr><td>1</td><td>Yve</td><td>1.0</td><td>CA</td></tr><tr><td>2</td><td>Omar</td><td>2.5</td><td>NY</td></tr><tr><td>3</td><td>Elia</td><td>3.3</td><td>OH</td></tr><tr><td>4</td><td>Rebecca</td><td>4.7</td><td>TX</td></tr><tr><td>5</td><td>Ameena</td><td>5.3</td><td>CA</td></tr><tr><td>6</td><td>Ling</td><td>6.6</td><td>NY</td></tr><tr><td>7</td><td>Pedro</td><td>7.1</td><td>KY</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Displayed results for the table demo_table\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Displayed results for the table demo_table\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"6efbda51-9c51-440a-aaaf-7276ad175398\"/>\n\n\n- On the first execution, the table **`demo_table`** did not yet exist. As such, the attempt to return the contents of the table created an error, which resulted in our **`except`** block of logic executing. This block:\n  1. Created the table\n  1. Inserted values\n  1. Printed or displayed the contents of the table\n- On the second execution, the table **`demo_table`** already exists, and so the first query in the **`try`** block executes without error. As a result, we just display the results of the query without modifying anything in our environment."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f0d4f22-19b1-438c-a180-2e4974e4b762","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"a0f957ea-7604-46b9-9b06-d672b73efcec\"/>\n\n\n## Adapting SQL to Python\nLet's consider the following SQL query against our demo table created above."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb83958a-1f73-42ae-bacc-a7a16e741e37","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT id, value \nFROM demo_table\nWHERE state = \"CA\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"706a27b9-999b-401a-9797-b78bf68e0a7e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,1.0],[5,5.3]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"value","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>value</th></tr></thead><tbody><tr><td>1</td><td>1.0</td></tr><tr><td>5</td><td>5.3</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"c4abcb35-3733-4565-8f8c-0df4b23f1e71\"/>\n\n\n\nwhich can also be expressed using the PySpark API and the **`display`** function as seen here:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d2c6d31-ad02-414a-90a2-8eb6fb12473e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["results = spark.sql(\"SELECT id, value FROM demo_table WHERE state = 'CA'\")\ndisplay(results)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82f51c79-6fba-4cd8-bee9-37e5a0c13701","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,1.0],[5,5.3]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"value","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>value</th></tr></thead><tbody><tr><td>1</td><td>1.0</td></tr><tr><td>5</td><td>5.3</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"6a4e7e96-c53a-4b8e-abf5-412fe4170c27\"/>\n\n\nLet's use this simple example to practice creating a Python function that adds optional functionality.\n\nOur target function will:\n* Be based upon a query that only includes the **`id`** and **`value`** columns from the a table named **`demo_table`**\n* Will allow filtering of that query by **`state`** where the the default behavior is to include all states\n* Will optionally render the results of the query using the **`display`** function where the default behavior is to not render\n* Will return:\n  * The query result object (a PySpark DataFrame) if **`render_results`** is False\n  * The **`None`** value  if **`render_results`** is True\n\nStretch Goal:\n* Add an assert statement to verify that the value passed for the **`state`** parameter contains two, uppercase letters\n\nSome starter logic has been provided below:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a59edec5-f73d-4967-86d5-95f85bf07cff","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# ANSWER\ndef preview_values(state=None, render_results=False):\n    query = \"SELECT id, value FROM demo_table\"\n    \n    if state is not None:\n        assert state == state.upper() and len(state) == 2, \"Please use the standard 2-letter, uppercase, state abbreviations\"\n        query += f\" WHERE state = '{state}'\"\n    \n    query_results = spark.sql(query)\n    \n    if render_results:\n        display(query_results)\n        return None\n    else:\n        return query_results"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"970e1c57-27db-43b8-a908-bf4a36239222","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"060207a1-a34c-4817-abee-f6e0b9c3b48a\"/>\n\n\nThe assert statements below can be used to check whether or not your function works as intended."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ee4ea91-ffb9-40d5-8d4c-376f916a805b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pyspark.sql.dataframe\n\nassert type(preview_values()) == pyspark.sql.dataframe.DataFrame, \"Function should return the results as a DataFrame\"\nassert preview_values().columns == [\"id\", \"value\"], \"Query should only return **`id`** and **`value`** columns\"\n\nassert preview_values(render_results=True) is None, \"Function should not return None when rendering\"\nassert preview_values(render_results=False) is not None, \"Function should return DataFrame when not rendering\"\n\nassert preview_values(state=None).count() == 7, \"Function should allow no state\"\nassert preview_values(state=\"NY\").count() == 2, \"Function should allow filtering by state\"\nassert preview_values(state=\"CA\").count() == 2, \"Function should allow filtering by state\"\nassert preview_values(state=\"OH\").count() == 1, \"Function should allow filtering by state\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32606df2-eac0-4d01-8e16-50b961cca5ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,1.0],[2,2.5],[3,3.3],[4,4.7],[5,5.3],[6,6.6],[7,7.1]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"value","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>value</th></tr></thead><tbody><tr><td>1</td><td>1.0</td></tr><tr><td>2</td><td>2.5</td></tr><tr><td>3</td><td>3.3</td></tr><tr><td>4</td><td>4.7</td></tr><tr><td>5</td><td>5.3</td></tr><tr><td>6</td><td>6.6</td></tr><tr><td>7</td><td>7.1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9b135c6e-1f8d-4bac-b8bb-5a3f085df950","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 5.3L - Python for SQL Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4094000743659921,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":4094000743659899}},"nbformat":4,"nbformat_minor":0}
