{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"474737ad-ef13-458c-9f35-4f3d99321815","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"469222b4-7728-4980-a105-a850e5234a3c\"/>\n\n\n# Providing Options for External Sources\nWhile directly querying files works well for self-describing formats, many data sources require additional configurations or schema declaration to properly ingest records.\n\nIn this lesson, we will create tables using external data sources. While these tables will not yet be stored in the Delta Lake format (and therefore not be optimized for the Lakehouse), this technique helps to facilitate extracting data from diverse external systems.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n- Use Spark SQL to configure options for extracting data from external sources\n- Create tables against external data sources for various file formats\n- Describe default behavior when querying tables defined against external sources"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff2856fa-0c06-4a6f-b3ac-3472b08e79c0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"b0456896-d5ed-416c-bfab-7d4f54a24288\"/>\n\n\n## Run Setup\n\nThe setup script will create the data and declare necessary values for the rest of this notebook to execute."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18e42117-7c93-4046-aadb-4df40692fe0f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-04.2"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"421edf43-49c7-4bc9-a984-56a2ec11d6ed","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(8 seconds)\n| completed (8 seconds total)\n\nCreating & using the schema \"munirsheikhcloudseekho_0lj9_da_dewd\"...(0 seconds)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(8 seconds)\n| completed (8 seconds total)\n\nCreating & using the schema \"munirsheikhcloudseekho_0lj9_da_dewd\"...(0 seconds)\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"},"removedWidgets":[],"addedWidgets":{},"metadata":{"kernelSessionId":"fe39b995-d61d4e702f0937df347a6616"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Creating the users table...","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Creating the users table..."]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"The execution of this command did not finish successfully","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"7ba475f1-5a89-46ee-b400-b8846699d5b5\"/>\n\n\n## When Direct Queries Don't Work \n\nWhile views can be used to persist direct queries against files between sessions, this approach has limited utility.\n\nCSV files are one of the most common file formats, but a direct query against these files rarely returns the desired results."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cb5e65f7-b6a4-4f2c-a0e6-da955e41978c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT * FROM csv.`${DA.paths.sales_csv}`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bb2900a1-03ea-4485-a2b7-d1badf072722","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Can not create a Path from an empty string; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:50)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:168)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1174)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1173)\n\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:254)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:176)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:93)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:51)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:317)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:221)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:317)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:297)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:296)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:349)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:777)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:349)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:973)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:346)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:132)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:105)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:973)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:808)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:973)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:803)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$23(DriverLocal.scala:725)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:41)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$20(DriverLocal.scala:708)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:398)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:396)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:393)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:441)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:426)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:685)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string\n\tat org.apache.hadoop.fs.Path.checkPathArg(Path.java:172)\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:184)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaRead(DeltaValidation.scala:130)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.org$apache$spark$sql$execution$datasources$ResolveSQLOnFile$$resolveDataSource(rules.scala:81)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:150)\n\t... 90 more\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$23(DriverLocal.scala:725)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:41)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$20(DriverLocal.scala:708)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:398)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:396)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:393)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:441)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:426)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:685)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"Error in SQL statement: AnalysisException: Can not create a Path from an empty string; line 1 pos 14","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: org.apache.spark.sql.AnalysisException: Can not create a Path from an empty string; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:50)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:168)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1174)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1173)\n\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:254)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:176)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:93)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:51)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:317)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:221)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:317)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:297)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:296)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:159)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:349)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:777)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:349)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:973)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:346)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:132)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:105)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:973)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:103)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:808)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:973)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:803)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$23(DriverLocal.scala:725)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:41)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$20(DriverLocal.scala:708)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:398)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:396)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:393)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:441)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:426)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:685)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string\n\tat org.apache.hadoop.fs.Path.checkPathArg(Path.java:172)\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:184)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaRead(DeltaValidation.scala:130)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.org$apache$spark$sql$execution$datasources$ResolveSQLOnFile$$resolveDataSource(rules.scala:81)\n\tat org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:150)\n\t... 90 more\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:130)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$23(DriverLocal.scala:725)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:41)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$20(DriverLocal.scala:708)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:398)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:396)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:393)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:441)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:426)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:62)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:685)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:622)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:614)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:533)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:568)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:232)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"7ae3fe20-46e7-42be-95d6-4f933b758ef7\"/>\n\n\nWe can see from the above that:\n1. The header row is being extracted as a table row\n1. All columns are being loaded as a single column\n1. The file is pipe-delimited (**`|`**)\n1. The final column appears to contain nested data that is being truncated"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1261f30e-6460-4c4f-b7ca-a7f705681bf3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"f371955b-5e96-41d3-b413-ed5bb51820fc\"/>\n\n\n## Registering Tables on External Data with Read Options\n\nWhile Spark will extract some self-describing data sources efficiently using default settings, many formats will require declaration of schema or other options.\n\nWhile there are many <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\" target=\"_blank\">additional configurations</a> you can set while creating tables against external sources, the syntax below demonstrates the essentials required to extract data from most formats.\n\n<strong><code>\nCREATE TABLE table_identifier (col_name1 col_type1, ...)<br/>\nUSING data_source<br/>\nOPTIONS (key1 = val1, key2 = val2, ...)<br/>\nLOCATION = path<br/>\n</code></strong>\n\nNote that options are passed with keys as unquoted text and values in quotes. Spark supports many <a href=\"https://docs.databricks.com/data/data-sources/index.html\" target=\"_blank\">data sources</a> with custom options, and additional systems may have unofficial support through external <a href=\"https://docs.databricks.com/libraries/index.html\" target=\"_blank\">libraries</a>. \n\n**NOTE**: Depending on your workspace settings, you may need administrator assistance to load libraries and configure the requisite security settings for some data sources."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a701c663-5380-4cc5-93cc-8b05b95c7b77","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"ea38261e-12c1-4bba-9670-9c4144c19494\"/>\n\n\nThe cell below demonstrates using Spark SQL DDL to create a table against an external CSV source, specifying:\n1. The column names and types\n1. The file format\n1. The delimiter used to separate fields\n1. The presence of a header\n1. The path to where this data is stored"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"118b8511-abdb-417f-abd0-0209f607f4b0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nCREATE TABLE sales_csv\n  (order_id LONG, email STRING, transactions_timestamp LONG, total_item_quantity INTEGER, purchase_revenue_in_usd DOUBLE, unique_items INTEGER, items STRING)\nUSING CSV\nOPTIONS (\n  header = \"true\",\n  delimiter = \"|\"\n)\nLOCATION \"${DA.paths.sales_csv}\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5bbe01af-b505-4e1e-829e-094820a2ab04","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"a9b747d0-1811-4bbe-b6d6-c89671289286\"/>\n\n\nNote that no data has moved during table declaration. \n\nSimilar to when we directly queried our files and created a view, we are still just pointing to files stored in an external location.\n\nRun the following cell to confirm that data is now being loaded correctly."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0dc329a0-3eae-4859-a70e-ca143ce8bc15","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT * FROM sales_csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b9fbc6c-bffd-48b0-87f3-4f559de98b1e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM sales_csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35daad32-36cd-49aa-bbeb-06db70d15a6d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"d0ad4268-b339-40cc-88be-1b07e9759d5e\"/>\n\n\nAll the metadata and options passed during table declaration will be persisted to the metastore, ensuring that data in the location will always be read with these options.\n\n**NOTE**: When working with CSVs as a data source, it's important to ensure that column order does not change if additional data files will be added to the source directory. Because the data format does not have strong schema enforcement, Spark will load columns and apply column names and data types in the order specified during table declaration.\n\nRunning **`DESCRIBE EXTENDED`** on a table will show all of the metadata associated with the table definition."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"622dfadb-52e7-4c39-9c55-277757eaf057","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED sales_csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3767a57d-e019-479d-9992-e385c5e84f0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"39ee6311-f095-4f86-8fe5-c5be569b6a10\"/>\n\n\n## Limits of Tables with External Data Sources\n\nIf you've taken other courses on Databricks or reviewed any of our company literature, you may have heard about Delta Lake and the Lakehouse. Note that whenever we're defining tables or queries against external data sources, we **cannot** expect the performance guarantees associated with Delta Lake and Lakehouse.\n\nFor example: while Delta Lake tables will guarantee that you always query the most recent version of your source data, tables registered against other data sources may represent older cached versions.\n\nThe cell below executes some logic that we can think of as just representing an external system directly updating the files underlying our table."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cea542d3-714c-411f-bc3b-f6d6e982b3c1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%python\n(spark.read\n      .option(\"header\", \"true\")\n      .option(\"delimiter\", \"|\")\n      .csv(DA.paths.sales_csv)\n      .write.mode(\"append\")\n      .format(\"csv\")\n      .save(DA.paths.sales_csv))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8189e292-1394-48e3-a03f-7fb3662a5f9a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"cad47cd2-c64d-4658-9b95-400ea7303f7f\"/>\n\n\nIf we look at the current count of records in our table, the number we see will not reflect these newly inserted rows."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50e525a2-fb82-44c3-bfc4-20326a3c211a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM sales_csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9c313a83-f5a0-48c2-b325-1104e7a7e8a2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"e9bb05a1-81db-4360-a131-bd14453af87e\"/>\n\n\nAt the time we previously queried this data source, Spark automatically cached the underlying data in local storage. This ensures that on subsequent queries, Spark will provide the optimal performance by just querying this local cache.\n\nOur external data source is not configured to tell Spark that it should refresh this data. \n\nWe **can** manually refresh the cache of our data by running the **`REFRESH TABLE`** command."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79957a53-d587-492e-9ff8-8aef121683be","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nREFRESH TABLE sales_csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6d446a4-deab-486b-bd4b-04e9aa7a2784","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"b7005568-3b03-4c4a-9ed0-122f14de6f7b\"/>\n\n\nNote that refreshing our table will invalidate our cache, meaning that we'll need to rescan our original data source and pull all data back into memory. \n\nFor very large datasets, this may take a significant amount of time."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32303d58-5e12-480e-8588-c199adeec6ef","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM sales_csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7b4eb4d5-bb41-44b1-8f2a-5f5fc02b5190","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"524835ab-53b7-435e-8c3a-9abcc81bab2e\"/>\n\n\n## Extracting Data from SQL Databases\nSQL databases are an extremely common data source, and Databricks has a standard JDBC driver for connecting with many flavors of SQL.\n\nThe general syntax for creating these connections is:\n\n<strong><code>\nCREATE TABLE <jdbcTable><br/>\nUSING JDBC<br/>\nOPTIONS (<br/>\n&nbsp; &nbsp; url = \"jdbc:{databaseServerType}://{jdbcHostname}:{jdbcPort}\",<br/>\n&nbsp; &nbsp; dbtable = \"{jdbcDatabase}.table\",<br/>\n&nbsp; &nbsp; user = \"{jdbcUsername}\",<br/>\n&nbsp; &nbsp; password = \"{jdbcPassword}\"<br/>\n)\n</code></strong>\n\nIn the code sample below, we'll connect with <a href=\"https://www.sqlite.org/index.html\" target=\"_blank\">SQLite</a>.\n  \n**NOTE:** SQLite uses a local file to store a database, and doesn't require a port, username, or password.  \n  \n<img src=\"https://files.training.databricks.com/images/icon_warn_24.png\"> **WARNING**: The backend-configuration of the JDBC server assume you are running this notebook on a single-node cluster. If you are running on a cluster with multiple workers, the client running in the executors will not be able to connect to the driver."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e1e74c3-38aa-4c45-a3a5-76661e237111","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS users_jdbc;\n\nCREATE TABLE users_jdbc\nUSING JDBC\nOPTIONS (\n  url = \"jdbc:sqlite:${DA.paths.ecommerce_db}\",\n  dbtable = \"users\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6af878fb-9009-4ef7-8b85-2c1ceeb9afff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"1f4c4ac6-dd8f-4198-a552-e2a65ce140d0\"/>\n\n\nNow we can query this table as if it were defined locally."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"409c2689-7b23-4a9a-a3fb-bfbbe851302d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT * FROM users_jdbc"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b56f0c6-2036-405c-9cfb-e0d95260adfb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"4d1ac331-1a39-4cca-b834-e4f848dd3e64\"/>\n\n\nLooking at the table metadata reveals that we have captured the schema information from the external system.\n\nStorage properties (which would include the username and password associated with the connection) are automatically redacted."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11994715-6bed-4e75-a084-56b6922cf0be","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED users_jdbc"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96d5a876-4ea2-47f2-84dd-316957e28500","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"2b8ce5b8-d76a-4be9-a2d3-9284ab701db7\"/>\n\n\nWhile the table is listed as **`MANAGED`**, listing the contents of the specified location confirms that no data is being persisted locally."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"061b505b-1734-4cf9-82e8-ec7f7fb223d3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%python\nimport pyspark.sql.functions as F\n\nlocation = spark.sql(\"DESCRIBE EXTENDED users_jdbc\").filter(F.col(\"col_name\") == \"Location\").first()[\"data_type\"]\nprint(location)\n\nfiles = dbutils.fs.ls(location)\nprint(f\"Found {len(files)} files\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a051aa2e-9acf-49a9-864d-53d4ec06b613","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"2a090fa9-b419-43cb-8249-0b0bc8f92918\"/>\n\n\nNote that some SQL systems such as data warehouses will have custom drivers. Spark will interact with various external databases differently, but the two basic approaches can be summarized as either:\n1. Moving the entire source table(s) to Databricks and then executing logic on the currently active cluster\n1. Pushing down the query to the external SQL database and only transferring the results back to Databricks\n\nIn either case, working with very large datasets in external SQL databases can incur significant overhead because of either:\n1. Network transfer latency associated with moving all data over the public internet\n1. Execution of query logic in source systems not optimized for big data queries"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6e2b881-228d-402e-93a4-0db0545fc87f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"7084cc69-4fea-45b7-9ab3-27ad8cd3e84c\"/>\n\n \nRun the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce9aa05a-13f1-4d36-9d04-b0fbd26721d4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%python\nDA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"173496a5-ada6-41fa-a61a-a028bb714ba5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b7aadeb9-d5fd-46f1-a9e1-88a85c7af09a","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 4.2 - Providing Options for External Sources","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4094000743659182}},"nbformat":4,"nbformat_minor":0}
